{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01a479a0-d77f-4bc8-b1d0-39490a36b137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the JSON file name (in the current directory):  output.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalizing table: CoffeeShopData\n",
      "Original table before normalization:\n",
      "\n",
      "Original for CoffeeShopData:\n",
      " OrderID    Date        PromocodeUsed TotalCost TotalDrinkCost TotalFoodCost  CustomerID CustomerName  DrinkID                DrinkName DrinkSize  DrinkQuantity Milk                        DrinkIngredient DrinkAllergen  FoodID          FoodName  FoodQuantity                    FoodIngredient FoodAllergen\n",
      "    1001 6/30/24                 NONE    $7.25          $7.25         $0.00            1  Alice Brown        1              Caffe Latte    Grande              1   ND                   {Espresso, Oat Milk}        {Oat}        0               NaN             0                              NONE         NONE\n",
      "    1002 6/30/26            SUMMERFUN    $9.98          $5.99         $3.99            2 David Miller        2   Iced Caramel Macchiato      Tall              2    D   {Expresso, Vanilla Syrup, Milk, Ice} {Dairy, Nuts}       3 Blueberry Muffin              1 {Flour, Sugar, Blueberries, Eggs} {Wheat, Egg}\n",
      "    1002 6/30/26            SUMMERFUN    $9.98          $5.99         $3.99            2 David Miller        3        Iced Matcha Latte    Grande              1   ND           {Matcha, Coconut Milk, Ice}         {Nuts}       3 Blueberry Muffin              1 {Flour, Sugar, Blueberries, Eggs} {Wheat, Egg}\n",
      "    1003 6/29/24 {SUMMERFUN, JUNEVIP}  $115.00        $115.00         $0.00            3 Emily Garcia        4 Vanilla Bean Frappuccino     Venti              8   ND {Coffee, Ice, Vanilla Syrup, Soy Milk}   {Nuts, Soy}       0               NaN             0                              NONE         NONE\n",
      "\n",
      "Table CoffeeShopData violates 1NF (non-atomic attributes: ['PromocodeUsed', 'DrinkIngredient', 'DrinkAllergen', 'FoodIngredient', 'FoodAllergen']). Decomposing.\n",
      "Decomposing non-atomic attribute: PromocodeUsed\n",
      "Generated table: CoffeeShopData1\n",
      "Decomposing non-atomic attribute: DrinkIngredient\n",
      "Generated table: CoffeeShopData2\n",
      "Decomposing non-atomic attribute: DrinkAllergen\n",
      "Generated table: CoffeeShopData3\n",
      "Decomposing non-atomic attribute: FoodIngredient\n",
      "Generated table: CoffeeShopData4\n",
      "Decomposing non-atomic attribute: FoodAllergen\n",
      "Generated table: CoffeeShopData5\n",
      "\n",
      "New table CoffeeShopData1 after applying 1NF:\n",
      " OrderID  DrinkID  FoodID PromocodeUsed\n",
      "    1001        1       0          NONE\n",
      "    1002        2       3     SUMMERFUN\n",
      "    1002        3       3     SUMMERFUN\n",
      "    1003        4       0     SUMMERFUN\n",
      "    1003        4       0       JUNEVIP\n",
      "\n",
      "New table CoffeeShopData2 after applying 1NF:\n",
      " OrderID  DrinkID  FoodID             DrinkIngredient\n",
      "    1001        1       0                    Espresso\n",
      "    1001        1       0                    Oat Milk\n",
      "    1002        2       3                    Expresso\n",
      "    1002        2       3               Vanilla Syrup\n",
      "    1002        2       3                        Milk\n",
      "    1002        2       3                         Ice\n",
      "    1002        3       3 {Matcha, Coconut Milk, Ice}\n",
      "    1003        4       0                      Coffee\n",
      "    1003        4       0                         Ice\n",
      "    1003        4       0               Vanilla Syrup\n",
      "    1003        4       0                    Soy Milk\n",
      "\n",
      "New table CoffeeShopData3 after applying 1NF:\n",
      " OrderID  DrinkID  FoodID DrinkAllergen\n",
      "    1001        1       0         {Oat}\n",
      "    1002        2       3         Dairy\n",
      "    1002        2       3          Nuts\n",
      "    1002        3       3          Nuts\n",
      "    1003        4       0          Nuts\n",
      "    1003        4       0           Soy\n",
      "\n",
      "New table CoffeeShopData4 after applying 1NF:\n",
      " OrderID  DrinkID  FoodID FoodIngredient\n",
      "    1001        1       0           NONE\n",
      "    1002        2       3          Flour\n",
      "    1002        2       3          Sugar\n",
      "    1002        2       3    Blueberries\n",
      "    1002        2       3           Eggs\n",
      "    1002        3       3          Flour\n",
      "    1002        3       3          Sugar\n",
      "    1002        3       3    Blueberries\n",
      "    1002        3       3           Eggs\n",
      "    1003        4       0           NONE\n",
      "\n",
      "New table CoffeeShopData5 after applying 1NF:\n",
      " OrderID  DrinkID  FoodID FoodAllergen\n",
      "    1001        1       0         NONE\n",
      "    1002        2       3        Wheat\n",
      "    1002        2       3          Egg\n",
      "    1002        3       3        Wheat\n",
      "    1002        3       3          Egg\n",
      "    1003        4       0         NONE\n",
      "\n",
      "Original for CoffeeShopData:\n",
      " OrderID    Date TotalCost TotalDrinkCost TotalFoodCost  CustomerID CustomerName  DrinkID                DrinkName DrinkSize  DrinkQuantity Milk  FoodID          FoodName  FoodQuantity\n",
      "    1001 6/30/24    $7.25          $7.25         $0.00            1  Alice Brown        1              Caffe Latte    Grande              1   ND       0               NaN             0\n",
      "    1002 6/30/26    $9.98          $5.99         $3.99            2 David Miller        2   Iced Caramel Macchiato      Tall              2    D       3 Blueberry Muffin              1\n",
      "    1002 6/30/26    $9.98          $5.99         $3.99            2 David Miller        3        Iced Matcha Latte    Grande              1   ND       3 Blueberry Muffin              1\n",
      "    1003 6/29/24  $115.00        $115.00         $0.00            3 Emily Garcia        4 Vanilla Bean Frappuccino     Venti              8   ND       0               NaN             0\n",
      "\n",
      "Table CoffeeShopData violates 2NF (partial dependency: {'lhs': ['OrderID'], 'rhs': ['Date', 'TotalCost', 'TotalDrinkCost', 'TotalFoodCost', 'CustomerID', 'CustomerName']}). Decomposing.\n",
      "Decomposed table CoffeeShopData_2NF_6:\n",
      "Attributes: ['OrderID', 'TotalFoodCost', 'TotalDrinkCost', 'Date', 'CustomerID', 'CustomerName', 'TotalCost']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Table CoffeeShopData violates 2NF (partial dependency: {'lhs': ['OrderID', 'DrinkID'], 'rhs': ['DrinkSize', 'DrinkQuantity', 'Milk']}). Decomposing.\n",
      "Decomposed table CoffeeShopData_2NF_7:\n",
      "Attributes: ['OrderID', 'DrinkSize', 'DrinkID', 'Milk', 'DrinkQuantity']\n",
      "Primary Key: ['OrderID', 'DrinkID']\n",
      "\n",
      "Table CoffeeShopData violates 2NF (partial dependency: {'lhs': ['OrderID', 'FoodID'], 'rhs': ['FoodQuantity']}). Decomposing.\n",
      "Decomposed table CoffeeShopData_2NF_8:\n",
      "Attributes: ['OrderID', 'FoodQuantity', 'FoodID']\n",
      "Primary Key: ['OrderID', 'FoodID']\n",
      "\n",
      "Table CoffeeShopData violates 2NF (partial dependency: {'lhs': ['DrinkID'], 'rhs': ['DrinkName']}). Decomposing.\n",
      "Decomposed table CoffeeShopData_2NF_9:\n",
      "Attributes: ['DrinkName', 'DrinkID']\n",
      "Primary Key: ['DrinkID']\n",
      "\n",
      "Table CoffeeShopData violates 2NF (partial dependency: {'lhs': ['FoodID'], 'rhs': ['FoodName']}). Decomposing.\n",
      "Decomposed table CoffeeShopData_2NF_10:\n",
      "Attributes: ['FoodName', 'FoodID']\n",
      "Primary Key: ['FoodID']\n",
      "\n",
      "Updating original table CoffeeShopData after decomposition:\n",
      "Updated original table CoffeeShopData:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID']\n",
      "Primary Key: ['OrderID', 'DrinkID', 'FoodID']\n",
      "All tables are in 2NF.\n",
      "\n",
      "Table CoffeeShopData_2NF_6 violates 3NF (transitive dependency: {'lhs': ['CustomerID'], 'rhs': ['CustomerName']}). Decomposing.\n",
      "Decomposed table CoffeeShopData_2NF_6_3NF_11:\n",
      "Attributes: ['CustomerName', 'CustomerID']\n",
      "Primary Key: ['CustomerID']\n",
      "All tables are in 3NF.\n",
      "All tables are in BCNF.\n",
      "\n",
      "Table CoffeeShopData violates 4NF (MVD: {'lhs': ['OrderID'], 'rhs': ['DrinkID']}). Decomposing.\n",
      "Decomposed table CoffeeShopData_4NF_12:\n",
      "Attributes: ['OrderID', 'DrinkID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Table CoffeeShopData violates 4NF (MVD: {'lhs': ['OrderID'], 'rhs': ['DrinkID', 'FoodID']}). Decomposing.\n",
      "Decomposed table CoffeeShopData_4NF_13:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Table CoffeeShopData1 violates 4NF (MVD: {'lhs': ['OrderID'], 'rhs': ['DrinkID']}). Decomposing.\n",
      "Decomposed table CoffeeShopData1_4NF_14:\n",
      "Attributes: ['OrderID', 'DrinkID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Table CoffeeShopData1 violates 4NF (MVD: {'lhs': ['OrderID'], 'rhs': ['DrinkID', 'FoodID']}). Decomposing.\n",
      "Decomposed table CoffeeShopData1_4NF_15:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Table CoffeeShopData2 violates 4NF (MVD: {'lhs': ['OrderID'], 'rhs': ['DrinkID']}). Decomposing.\n",
      "Decomposed table CoffeeShopData2_4NF_16:\n",
      "Attributes: ['OrderID', 'DrinkID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Table CoffeeShopData2 violates 4NF (MVD: {'lhs': ['OrderID'], 'rhs': ['DrinkID', 'FoodID']}). Decomposing.\n",
      "Decomposed table CoffeeShopData2_4NF_17:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Table CoffeeShopData3 violates 4NF (MVD: {'lhs': ['OrderID'], 'rhs': ['DrinkID']}). Decomposing.\n",
      "Decomposed table CoffeeShopData3_4NF_18:\n",
      "Attributes: ['OrderID', 'DrinkID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Table CoffeeShopData3 violates 4NF (MVD: {'lhs': ['OrderID'], 'rhs': ['DrinkID', 'FoodID']}). Decomposing.\n",
      "Decomposed table CoffeeShopData3_4NF_19:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Table CoffeeShopData4 violates 4NF (MVD: {'lhs': ['OrderID'], 'rhs': ['DrinkID']}). Decomposing.\n",
      "Decomposed table CoffeeShopData4_4NF_20:\n",
      "Attributes: ['OrderID', 'DrinkID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Table CoffeeShopData4 violates 4NF (MVD: {'lhs': ['OrderID'], 'rhs': ['DrinkID', 'FoodID']}). Decomposing.\n",
      "Decomposed table CoffeeShopData4_4NF_21:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Table CoffeeShopData5 violates 4NF (MVD: {'lhs': ['OrderID'], 'rhs': ['DrinkID']}). Decomposing.\n",
      "Decomposed table CoffeeShopData5_4NF_22:\n",
      "Attributes: ['OrderID', 'DrinkID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Table CoffeeShopData5 violates 4NF (MVD: {'lhs': ['OrderID'], 'rhs': ['DrinkID', 'FoodID']}). Decomposing.\n",
      "Decomposed table CoffeeShopData5_4NF_23:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Table CoffeeShopData_2NF_6 violates 4NF (MVD: {'lhs': ['OrderID'], 'rhs': ['DrinkID']}). Decomposing.\n",
      "Decomposed table CoffeeShopData_2NF_6_4NF_24:\n",
      "Attributes: ['OrderID', 'DrinkID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Table CoffeeShopData_2NF_6 violates 4NF (MVD: {'lhs': ['OrderID'], 'rhs': ['DrinkID', 'FoodID']}). Decomposing.\n",
      "Decomposed table CoffeeShopData_2NF_6_4NF_25:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Table CoffeeShopData_2NF_7 violates 4NF (MVD: {'lhs': ['OrderID'], 'rhs': ['DrinkID']}). Decomposing.\n",
      "Decomposed table CoffeeShopData_2NF_7_4NF_26:\n",
      "Attributes: ['OrderID', 'DrinkID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Table CoffeeShopData_2NF_7 violates 4NF (MVD: {'lhs': ['OrderID'], 'rhs': ['DrinkID', 'FoodID']}). Decomposing.\n",
      "Decomposed table CoffeeShopData_2NF_7_4NF_27:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Table CoffeeShopData_2NF_8 violates 4NF (MVD: {'lhs': ['OrderID'], 'rhs': ['DrinkID']}). Decomposing.\n",
      "Decomposed table CoffeeShopData_2NF_8_4NF_28:\n",
      "Attributes: ['OrderID', 'DrinkID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Table CoffeeShopData_2NF_8 violates 4NF (MVD: {'lhs': ['OrderID'], 'rhs': ['DrinkID', 'FoodID']}). Decomposing.\n",
      "Decomposed table CoffeeShopData_2NF_8_4NF_29:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Table CoffeeShopData_2NF_9 violates 4NF (MVD: {'lhs': ['OrderID'], 'rhs': ['DrinkID']}). Decomposing.\n",
      "Decomposed table CoffeeShopData_2NF_9_4NF_30:\n",
      "Attributes: ['OrderID', 'DrinkID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Table CoffeeShopData_2NF_9 violates 4NF (MVD: {'lhs': ['OrderID'], 'rhs': ['DrinkID', 'FoodID']}). Decomposing.\n",
      "Decomposed table CoffeeShopData_2NF_9_4NF_31:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Table CoffeeShopData_2NF_10 violates 4NF (MVD: {'lhs': ['OrderID'], 'rhs': ['DrinkID']}). Decomposing.\n",
      "Decomposed table CoffeeShopData_2NF_10_4NF_32:\n",
      "Attributes: ['OrderID', 'DrinkID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Table CoffeeShopData_2NF_10 violates 4NF (MVD: {'lhs': ['OrderID'], 'rhs': ['DrinkID', 'FoodID']}). Decomposing.\n",
      "Decomposed table CoffeeShopData_2NF_10_4NF_33:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Table CoffeeShopData_2NF_6_3NF_11 violates 4NF (MVD: {'lhs': ['OrderID'], 'rhs': ['DrinkID']}). Decomposing.\n",
      "Decomposed table CoffeeShopData_2NF_6_3NF_11_4NF_34:\n",
      "Attributes: ['OrderID', 'DrinkID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Table CoffeeShopData_2NF_6_3NF_11 violates 4NF (MVD: {'lhs': ['OrderID'], 'rhs': ['DrinkID', 'FoodID']}). Decomposing.\n",
      "Decomposed table CoffeeShopData_2NF_6_3NF_11_4NF_35:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Checking table CoffeeShopData for 5NF violations.\n",
      "Checking table CoffeeShopData1 for join dependencies.\n",
      "Checking table CoffeeShopData2 for join dependencies.\n",
      "Checking table CoffeeShopData3 for join dependencies.\n",
      "Checking table CoffeeShopData4 for join dependencies.\n",
      "Checking table CoffeeShopData5 for join dependencies.\n",
      "Checking table CoffeeShopData_2NF_6 for join dependencies.\n",
      "Checking table CoffeeShopData_2NF_7 for join dependencies.\n",
      "Checking table CoffeeShopData_2NF_8 for join dependencies.\n",
      "Checking table CoffeeShopData_2NF_9 for join dependencies.\n",
      "Checking table CoffeeShopData_2NF_10 for join dependencies.\n",
      "Checking table CoffeeShopData_2NF_6_3NF_11 for join dependencies.\n",
      "Checking table CoffeeShopData_4NF_12 for join dependencies.\n",
      "Checking table CoffeeShopData_4NF_13 for join dependencies.\n",
      "Checking table CoffeeShopData1_4NF_14 for join dependencies.\n",
      "Checking table CoffeeShopData1_4NF_15 for join dependencies.\n",
      "Checking table CoffeeShopData2_4NF_16 for join dependencies.\n",
      "Checking table CoffeeShopData2_4NF_17 for join dependencies.\n",
      "Checking table CoffeeShopData3_4NF_18 for join dependencies.\n",
      "Checking table CoffeeShopData3_4NF_19 for join dependencies.\n",
      "Checking table CoffeeShopData4_4NF_20 for join dependencies.\n",
      "Checking table CoffeeShopData4_4NF_21 for join dependencies.\n",
      "Checking table CoffeeShopData5_4NF_22 for join dependencies.\n",
      "Checking table CoffeeShopData5_4NF_23 for join dependencies.\n",
      "Checking table CoffeeShopData_2NF_6_4NF_24 for join dependencies.\n",
      "Checking table CoffeeShopData_2NF_6_4NF_25 for join dependencies.\n",
      "Checking table CoffeeShopData_2NF_7_4NF_26 for join dependencies.\n",
      "Checking table CoffeeShopData_2NF_7_4NF_27 for join dependencies.\n",
      "Checking table CoffeeShopData_2NF_8_4NF_28 for join dependencies.\n",
      "Checking table CoffeeShopData_2NF_8_4NF_29 for join dependencies.\n",
      "Checking table CoffeeShopData_2NF_9_4NF_30 for join dependencies.\n",
      "Checking table CoffeeShopData_2NF_9_4NF_31 for join dependencies.\n",
      "Checking table CoffeeShopData_2NF_10_4NF_32 for join dependencies.\n",
      "Checking table CoffeeShopData_2NF_10_4NF_33 for join dependencies.\n",
      "Checking table CoffeeShopData_2NF_6_3NF_11_4NF_34 for join dependencies.\n",
      "Checking table CoffeeShopData_2NF_6_3NF_11_4NF_35 for join dependencies.\n",
      "All join dependencies satisfied for table CoffeeShopData.\n",
      "All tables are already in 5NF.\n",
      "\n",
      "All decompositions for each normal form level:\n",
      "\n",
      "Decompositions after applying 1NF:\n",
      "Decomposed table CoffeeShopData1_1NF:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID', 'PromocodeUsed']\n",
      "Primary Key: ['OrderID', 'DrinkID', 'FoodID']\n",
      "\n",
      "Decomposed table CoffeeShopData2_1NF:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID', 'DrinkIngredient']\n",
      "Primary Key: ['OrderID', 'DrinkID', 'FoodID']\n",
      "\n",
      "Decomposed table CoffeeShopData3_1NF:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID', 'DrinkAllergen']\n",
      "Primary Key: ['OrderID', 'DrinkID', 'FoodID']\n",
      "\n",
      "Decomposed table CoffeeShopData4_1NF:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID', 'FoodIngredient']\n",
      "Primary Key: ['OrderID', 'DrinkID', 'FoodID']\n",
      "\n",
      "Decomposed table CoffeeShopData5_1NF:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID', 'FoodAllergen']\n",
      "Primary Key: ['OrderID', 'DrinkID', 'FoodID']\n",
      "\n",
      "\n",
      "Decompositions after applying 2NF:\n",
      "Decomposed table CoffeeShopData_2NF_6_2NF:\n",
      "Attributes: ['OrderID', 'TotalFoodCost', 'TotalDrinkCost', 'Date', 'CustomerID', 'TotalCost']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Decomposed table CoffeeShopData_2NF_7_2NF:\n",
      "Attributes: ['OrderID', 'DrinkSize', 'Milk', 'DrinkQuantity']\n",
      "Primary Key: ['OrderID', 'DrinkID']\n",
      "\n",
      "Decomposed table CoffeeShopData_2NF_8_2NF:\n",
      "Attributes: ['OrderID', 'FoodQuantity']\n",
      "Primary Key: ['OrderID', 'FoodID']\n",
      "\n",
      "Decomposed table CoffeeShopData_2NF_9_2NF:\n",
      "Attributes: ['DrinkName']\n",
      "Primary Key: ['DrinkID']\n",
      "\n",
      "Decomposed table CoffeeShopData_2NF_10_2NF:\n",
      "Attributes: ['FoodName']\n",
      "Primary Key: ['FoodID']\n",
      "\n",
      "\n",
      "Decompositions after applying 3NF:\n",
      "Decomposed table CoffeeShopData_2NF_6_3NF_11_3NF:\n",
      "Attributes: ['CustomerName', 'CustomerID']\n",
      "Primary Key: ['CustomerID']\n",
      "\n",
      "\n",
      "Decompositions after applying BCNF:\n",
      "\n",
      "Decompositions after applying 4NF:\n",
      "Decomposed table CoffeeShopData_4NF_12_4NF:\n",
      "Attributes: ['OrderID', 'DrinkID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Decomposed table CoffeeShopData_4NF_13_4NF:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Decomposed table CoffeeShopData1_4NF_14_4NF:\n",
      "Attributes: ['OrderID', 'DrinkID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Decomposed table CoffeeShopData1_4NF_15_4NF:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Decomposed table CoffeeShopData2_4NF_16_4NF:\n",
      "Attributes: ['OrderID', 'DrinkID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Decomposed table CoffeeShopData2_4NF_17_4NF:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Decomposed table CoffeeShopData3_4NF_18_4NF:\n",
      "Attributes: ['OrderID', 'DrinkID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Decomposed table CoffeeShopData3_4NF_19_4NF:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Decomposed table CoffeeShopData4_4NF_20_4NF:\n",
      "Attributes: ['OrderID', 'DrinkID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Decomposed table CoffeeShopData4_4NF_21_4NF:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Decomposed table CoffeeShopData5_4NF_22_4NF:\n",
      "Attributes: ['OrderID', 'DrinkID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Decomposed table CoffeeShopData5_4NF_23_4NF:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Decomposed table CoffeeShopData_2NF_6_4NF_24_4NF:\n",
      "Attributes: ['OrderID', 'DrinkID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Decomposed table CoffeeShopData_2NF_6_4NF_25_4NF:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Decomposed table CoffeeShopData_2NF_7_4NF_26_4NF:\n",
      "Attributes: ['OrderID', 'DrinkID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Decomposed table CoffeeShopData_2NF_7_4NF_27_4NF:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Decomposed table CoffeeShopData_2NF_8_4NF_28_4NF:\n",
      "Attributes: ['OrderID', 'DrinkID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Decomposed table CoffeeShopData_2NF_8_4NF_29_4NF:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Decomposed table CoffeeShopData_2NF_9_4NF_30_4NF:\n",
      "Attributes: ['OrderID', 'DrinkID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Decomposed table CoffeeShopData_2NF_9_4NF_31_4NF:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Decomposed table CoffeeShopData_2NF_10_4NF_32_4NF:\n",
      "Attributes: ['OrderID', 'DrinkID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Decomposed table CoffeeShopData_2NF_10_4NF_33_4NF:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Decomposed table CoffeeShopData_2NF_6_3NF_11_4NF_34_4NF:\n",
      "Attributes: ['OrderID', 'DrinkID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "Decomposed table CoffeeShopData_2NF_6_3NF_11_4NF_35_4NF:\n",
      "Attributes: ['OrderID', 'DrinkID', 'FoodID']\n",
      "Primary Key: ['OrderID']\n",
      "\n",
      "\n",
      "Decompositions after applying 5NF:\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "class Normalizer:\n",
    "    def __init__(self, relations, fds, mvds, target_nf):\n",
    "        self.relations = relations\n",
    "        self.fds = fds\n",
    "        self.mvds = mvds\n",
    "        self.target_nf = target_nf\n",
    "        self.normalized_relations = []\n",
    "        self.table_counter = 1\n",
    "        self.join_dependencies = []\n",
    "\n",
    "    def identify_dependencies(self):\n",
    "      \n",
    "        for fd in self.fds:\n",
    "            lhs = fd['lhs']\n",
    "            rhs = fd['rhs']\n",
    "            if len(lhs) > 1 and len(rhs) > 1:\n",
    "                self.join_dependencies.append({\n",
    "                    'lhs': lhs,\n",
    "                    'rhs': rhs\n",
    "                })\n",
    "\n",
    "    def normalize(self):\n",
    "\n",
    "        # dependencies before starting normalization\n",
    "        self.identify_dependencies()\n",
    "\n",
    "\n",
    "        for relation in self.relations:\n",
    "            print(f\"\\nNormalizing table: {relation['table_name']}\")\n",
    "            \n",
    "            # Track decompositions\n",
    "            nf_history = {'1NF': [], '2NF': [], '3NF': [], 'BCNF': [], '4NF': [], '5NF': []}\n",
    "\n",
    "            # Display the original relation\n",
    "            orginal_table = relation.copy()\n",
    "            print(\"Original table before normalization:\")\n",
    "            self.print_table(relation, \"Original\")\n",
    "\n",
    "            # Always apply 1NF\n",
    "            self.apply_1NF(relation)\n",
    "            #self.decompose_1NF(relation)\n",
    "            #self.display_and_store_decompositions(relation, nf_history, \"1NF\")\n",
    "            if self.apply_1NF:\n",
    "                self.display_and_store_decompositions(relation, nf_history, \"1NF\")\n",
    "                self.print_table(relation, \"Original\")\n",
    "\n",
    "            if self.target_nf == \"1NF\":\n",
    "                continue  # Stop after applying 1NF\n",
    "\n",
    "            # Apply 2NF and display tables if needed\n",
    "            if self.target_nf in [\"2NF\", \"3NF\", \"BCNF\", \"4NF\", \"5NF\"]:\n",
    "                #self.apply_2NF(relation)\n",
    "                self.apply_2NF(relation, nf_history)\n",
    "                #print(\"Your intial table's highest normal form was 1NF.\")\n",
    "                if not self.apply_2NF:\n",
    "                    self.display_and_store_decompositions(relation, nf_history, \"2NF\")\n",
    "                    self.print_table(relation, \"Original\")\n",
    "\n",
    "            if self.target_nf == \"2NF\":\n",
    "                continue  # Stop after applying 2NF\n",
    "\n",
    "            # Apply 3NF and display tables if needed\n",
    "            if self.target_nf in [\"3NF\", \"BCNF\", \"4NF\", \"5NF\"]:\n",
    "                self.apply_3NF(relation, nf_history)\n",
    "                if not self.apply_3NF:\n",
    "                    self.display_and_store_decompositions(relation, nf_history, \"3NF\")\n",
    "               \n",
    "\n",
    "            if self.target_nf == \"3NF\":\n",
    "                continue  # Stop after applying 3NF\n",
    "\n",
    "            # Apply BCNF and display tables if needed\n",
    "            if self.target_nf in [\"BCNF\", \"4NF\", \"5NF\"]:\n",
    "                self.apply_bcnf(relation, nf_history)\n",
    "                if not self.apply_bcnf:\n",
    "                    self.display_and_store_decompositions(relation, nf_history, \"BCNF\")\n",
    "              \n",
    "\n",
    "            if self.target_nf == \"BCNF\":\n",
    "                continue  # Stop after applying BCNF\n",
    "\n",
    "            if self.target_nf in [\"4NF\", \"5NF\"]:\n",
    "                self.apply_4NF(relation, nf_history)\n",
    "                if not self.apply_4NF:\n",
    "                    self.display_and_store_decompositions(relation, nf_history, \"4NF\")\n",
    "                         \n",
    "\n",
    "            # Apply 4NF if target is 4NF\n",
    "            if self.target_nf == \"4NF\":\n",
    "                continue\n",
    "\n",
    "            if self.target_nf == \"5NF\":\n",
    "                self.apply_5NF(relation, nf_history)\n",
    "                if not self.apply_5NF:\n",
    "                    self.display_and_store_decompositions(relation, nf_history, \"5NF\")\n",
    "                else:\n",
    "                    print(\"All tables are already in 5NF.\")\n",
    "            if self.target_nf == \"5NF\":\n",
    "                continue\n",
    "\n",
    "        \n",
    "        print(\"\\nAll decompositions for each normal form level:\")\n",
    "        for nf_level, tables in nf_history.items():\n",
    "            print(f\"\\nDecompositions after applying {nf_level}:\")\n",
    "            for table in tables:\n",
    "                # Print the table name with normal form levels applied\n",
    "                table_name = f\"{table['table_name']}_{nf_level}\"\n",
    "                \n",
    "                # Output the table's name, attributes, and primary key\n",
    "                print(f\"Decomposed table {table_name}:\")\n",
    "                print(f\"Attributes: {table['attributes']}\")\n",
    "                print(f\"Primary Key: {table['primary_key']}\\n\")\n",
    "\n",
    "        return self.normalized_relations\n",
    "\n",
    "    def display_and_store_decompositions(self, relation, nf_history, nf_level):\n",
    "       \n",
    "        decompositions = []\n",
    "        for table in self.normalized_relations:\n",
    "            if table['table_name'].startswith(relation['table_name']):\n",
    "                # Store a copy of the table to preserve current state\n",
    "                decompositions.append(table.copy())\n",
    "                print(f\"\\nNew table {table['table_name']} after applying {nf_level}:\")\n",
    "                df = pd.DataFrame(table['rows'], columns=table['attributes'])\n",
    "                print(df.to_string(index=False))\n",
    "        \n",
    "        # Append decompositions to history without clearing normalized relations\n",
    "        nf_history[nf_level].extend(decompositions)\n",
    "\n",
    "    def split_rows(self, rows, non_atomic_attrs, attributes):\n",
    "        split_rows = []\n",
    "\n",
    "        # Convert rows from list of lists to list of dicts for easier processing\n",
    "        dict_rows = [dict(zip(attributes, row)) for row in rows]\n",
    "\n",
    "        for row in dict_rows:\n",
    "            # Get base row containing only primary keys and non-non_atomic attributes\n",
    "            base_row = {attr: row.get(attr, None) for attr in attributes if attr not in non_atomic_attrs}\n",
    "            \n",
    "            # Process each non-atomic attribute\n",
    "            for attr in non_atomic_attrs:\n",
    "                if attr in row and '{' in row[attr]:  # Confirm it's non-atomic\n",
    "                    # Split the value of the non-atomic attribute\n",
    "                    values = self.Get_values(row[attr])\n",
    "                    for value in values:\n",
    "                        new_row = base_row.copy()\n",
    "                        new_row[attr] = value.strip()  # Ensure individual atomic value\n",
    "                        split_rows.append(new_row)\n",
    "                else:\n",
    "                    # If attribute is already atomic, simply add it to the base row\n",
    "                    new_row = base_row.copy()\n",
    "                    new_row[attr] = row.get(attr, None)\n",
    "                    split_rows.append(new_row)\n",
    "\n",
    "        return split_rows\n",
    "\n",
    "    def Get_values(self, non_atomic_value):\n",
    "        # Ensure the value is always treated as a set of individual elements\n",
    "        if isinstance(non_atomic_value, str) and non_atomic_value.startswith('{') and non_atomic_value.endswith('}'):\n",
    "            inner_content = non_atomic_value[1:-1]  # Remove braces\n",
    "            return [value.strip() for value in inner_content.split(',')]\n",
    "        elif isinstance(non_atomic_value, str):  # Handle values without braces\n",
    "            return [non_atomic_value.strip()]\n",
    "        else:\n",
    "            return [str(non_atomic_value).strip()]  # Fallback for any unexpected format\n",
    "\n",
    "\n",
    "    def Get_rows(self, rows, lhs, rhs):\n",
    "      \n",
    "        extracted_rows = []\n",
    "        for row in rows:\n",
    "            # Create a subset of row data for the new table with only lhs + rhs attributes\n",
    "            new_row = {attr: row[attr] for attr in lhs + rhs if attr in row}\n",
    "            if new_row:  # Ensure new_row is non-empty\n",
    "                extracted_rows.append(new_row)\n",
    "        return extracted_rows\n",
    "\n",
    "    def apply_1NF(self, relation):\n",
    "       \n",
    "        non_atomic = relation.get(\"non_atomic\", [])\n",
    "        decomposed_tables = []\n",
    "        \n",
    "        if non_atomic:\n",
    "            print(f\"\\nTable {relation['table_name']} violates 1NF (non-atomic attributes: {non_atomic}). Decomposing.\")\n",
    "            \n",
    "            for attr in non_atomic:\n",
    "                new_attributes = relation['primary_key'] + [attr]\n",
    "                split_rows = self.split_rows(relation['rows'], [attr], relation['attributes'])  # Pass the attributes\n",
    "                new_relation = {\n",
    "                    'table_name': f\"{relation['table_name']}{self.table_counter}\",\n",
    "                    'attributes': new_attributes,\n",
    "                    'rows': split_rows,\n",
    "                    'primary_key': relation['primary_key']\n",
    "                }\n",
    "                decomposed_tables.append(new_relation)\n",
    "                self.normalized_relations.append(new_relation)\n",
    "                self.table_counter += 1\n",
    "                \n",
    "                # DEBUG\n",
    "                print(\"Decomposing non-atomic attribute:\", attr)\n",
    "                print(\"Generated table:\", new_relation['table_name'])\n",
    "            \n",
    "            # Identify atomic attribute indices and reformat rows\n",
    "            atomic_attributes = [attr for attr in relation['attributes'] if attr not in non_atomic]\n",
    "            atomic_indices = [relation['attributes'].index(attr) for attr in atomic_attributes]\n",
    "            \n",
    "            # Reformat rows to include only atomic attribute values in correct order\n",
    "            relation['rows'] = [[row[idx] for idx in atomic_indices] for row in relation['rows']]\n",
    "            \n",
    "            # Update attributes to reflect only atomic attributes\n",
    "            relation['attributes'] = atomic_attributes\n",
    "\n",
    "            return decomposed_tables, True\n",
    "        \n",
    "        else:\n",
    "            print(f\"\\nTable {relation['table_name']} does not violate 1NF.\")\n",
    "            return False\n",
    "\n",
    "    def apply_2NF(self, relation, nf_history):\n",
    "        original_table_name = relation['table_name']  # Store original table name for reference\n",
    "        all_tables = [relation] + self.normalized_relations  # Include original and 1NF decompositions\n",
    "        decomposed_tables = []\n",
    "        removed_attributes = set()  # Track attributes removed from the original table\n",
    "\n",
    "        for table in all_tables:\n",
    "            # Check if the current table is the original table\n",
    "            is_original_table = (table['table_name'] == original_table_name)\n",
    "            \n",
    "            partial_dependency_found = False  # Reset for each table\n",
    "            relevant_fds = [\n",
    "                fd for fd in self.fds\n",
    "                if set(fd['lhs']).issubset(set(table['attributes'])) and set(fd['rhs']).issubset(set(table['attributes']))\n",
    "            ]\n",
    "\n",
    "            for fd in relevant_fds:\n",
    "                lhs, rhs = fd['lhs'], fd['rhs']\n",
    "                primary_key = table['primary_key']\n",
    "\n",
    "                # Check for a partial dependency\n",
    "                if set(lhs).issubset(set(primary_key)) and not set(rhs).issubset(set(primary_key)):\n",
    "                    partial_dependency_found = True\n",
    "                    print(f\"\\nTable {table['table_name']} violates 2NF (partial dependency: {fd}). Decomposing.\")\n",
    "\n",
    "                    # Create new relation based on partial dependency\n",
    "                    new_relation = {\n",
    "                        'table_name': f\"{table['table_name']}_2NF_{self.table_counter}\",\n",
    "                        'attributes': list(set(lhs + rhs)),\n",
    "                        'primary_key': lhs,\n",
    "                        'rows': self.Get_rows(table['rows'], lhs, rhs),\n",
    "                    }\n",
    "                    decomposed_tables.append(new_relation)\n",
    "                    self.normalized_relations.append(new_relation)\n",
    "                    self.table_counter += 1\n",
    "\n",
    "                    # Display the new decomposed table immediately after creation\n",
    "                    print(f\"Decomposed table {new_relation['table_name']}:\")\n",
    "                    print(\"Attributes:\", new_relation['attributes'])\n",
    "                    #print(\"Rows:\", new_relation['rows'])\n",
    "                    print(\"Primary Key:\", new_relation['primary_key'])\n",
    "\n",
    "                    nf_history['2NF'].append(new_relation)\n",
    "\n",
    "\n",
    "                    # Remove RHS attributes of this dependency from the current table and track removed attributes\n",
    "                    for attr in rhs:\n",
    "                        if attr not in removed_attributes:\n",
    "                            removed_attributes.add(attr)\n",
    "                            table['attributes'].remove(attr)\n",
    "\n",
    "            # Update rows of the original table only if a partial dependency was found\n",
    "            if partial_dependency_found and is_original_table:\n",
    "                print(f\"\\nUpdating original table {table['table_name']} after decomposition:\")\n",
    "                table['rows'] = [\n",
    "                    [value for i, value in enumerate(row) if i < len(table['attributes']) and table['attributes'][i] not in removed_attributes]\n",
    "                    for row in table['rows']\n",
    "                ]\n",
    "\n",
    "                # Output the updated original table after attribute removal\n",
    "                print(f\"Updated original table {table['table_name']}:\")\n",
    "                print(\"Attributes:\", table['attributes'])\n",
    "                #print(\"Rows:\", table['rows'])\n",
    "                print(\"Primary Key:\", table['primary_key'])\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(\"All tables are in 2NF.\")\n",
    "\n",
    "        return decomposed_tables if decomposed_tables else False\n",
    "\n",
    "    def remove_partial_rows(self, rows, decomposed_tables, primary_key):\n",
    "       \n",
    "        # Use the original table's attributes to get indices\n",
    "        original_attributes = decomposed_tables[0]['attributes']  # Replace with the original table's attributes list\n",
    "        key_indices = [original_attributes.index(attr) for attr in primary_key if attr in original_attributes]\n",
    "\n",
    "        filtered_rows = []\n",
    "        seen_keys = set()\n",
    "\n",
    "        for row in rows:\n",
    "            # Generate the primary key tuple based on indices\n",
    "            row_key = tuple(row[idx] for idx in key_indices if idx < len(row))\n",
    "            \n",
    "            # Only add rows with unique primary keys\n",
    "            if row_key not in seen_keys:\n",
    "                seen_keys.add(row_key)\n",
    "                filtered_rows.append(row)\n",
    "\n",
    "        return filtered_rows\n",
    "\n",
    "    def apply_3NF(self, relation, nf_history):\n",
    "        original_table_name = relation['table_name']  # Store original table name for reference\n",
    "        all_tables = [relation] + self.normalized_relations  # Include original and 2NF decompositions\n",
    "        decomposed_tables = []\n",
    "        removed_attributes = set()  # Track attributes removed from the original table\n",
    "\n",
    "        for table in all_tables:\n",
    "            # Check if the current table is the original table\n",
    "            is_original_table = (table['table_name'] == original_table_name)\n",
    "            \n",
    "            transitive_dependency_found = False  # Reset for each table\n",
    "            relevant_fds = [\n",
    "                fd for fd in self.fds\n",
    "                if set(fd['lhs']).issubset(set(table['attributes'])) and set(fd['rhs']).issubset(set(table['attributes']))\n",
    "            ]\n",
    "\n",
    "            for fd in relevant_fds:\n",
    "                lhs, rhs = fd['lhs'], fd['rhs']\n",
    "                primary_key = table['primary_key']\n",
    "\n",
    "                # Check for a transitive dependency\n",
    "                if not set(lhs).issubset(set(primary_key)) and any(attr not in primary_key for attr in rhs):\n",
    "                    transitive_dependency_found = True\n",
    "                    print(f\"\\nTable {table['table_name']} violates 3NF (transitive dependency: {fd}). Decomposing.\")\n",
    "\n",
    "                    # Create new relation based on transitive dependency\n",
    "                    new_relation = {\n",
    "                        'table_name': f\"{table['table_name']}_3NF_{self.table_counter}\",\n",
    "                        'attributes': list(set(lhs + rhs)),\n",
    "                        'primary_key': lhs,\n",
    "                        'rows': self.Get_rows(table['rows'], lhs, rhs),\n",
    "                    }\n",
    "                    decomposed_tables.append(new_relation)\n",
    "                    self.normalized_relations.append(new_relation)\n",
    "                    self.table_counter += 1\n",
    "\n",
    " \n",
    "                    print(f\"Decomposed table {new_relation['table_name']}:\")\n",
    "                    print(\"Attributes:\", new_relation['attributes'])\n",
    "                    #print(\"Rows:\", new_relation['rows'])\n",
    "                    print(\"Primary Key:\", new_relation['primary_key'])\n",
    "\n",
    "                    nf_history['3NF'].append(new_relation)\n",
    "\n",
    "                    # Remove RHS attributes of this dependency from the current table and track removed attributes\n",
    "                    for attr in rhs:\n",
    "                        if attr not in removed_attributes:\n",
    "                            removed_attributes.add(attr)\n",
    "                            table['attributes'].remove(attr)\n",
    "\n",
    "            # Update rows of the original table only if a transitive dependency was found\n",
    "            if transitive_dependency_found and is_original_table:\n",
    "                print(f\"\\nUpdating original table {table['table_name']} after decomposition:\")\n",
    "                table['rows'] = [\n",
    "                    [value for i, value in enumerate(row) if i < len(table['attributes']) and table['attributes'][i] not in removed_attributes]\n",
    "                    for row in table['rows']\n",
    "                ]\n",
    "\n",
    "                # Output the updated original table after attribute removal\n",
    "                print(f\"Updated original table {table['table_name']}:\")\n",
    "                print(\"Attributes:\", table['attributes'])\n",
    "                #print(\"Rows:\", table['rows'])\n",
    "                print(\"Primary Key:\", table['primary_key'])\n",
    "\n",
    "        else:\n",
    "            print(\"All tables are in 3NF.\")\n",
    "\n",
    "        return decomposed_tables if decomposed_tables else False\n",
    "\n",
    "    def apply_bcnf(self, relation, nf_history):\n",
    "        original_table_name = relation['table_name']  # Store original table name for reference\n",
    "        all_tables = [relation] + self.normalized_relations  # Include original and prior decompositions\n",
    "        decomposed_tables = []\n",
    "        removed_attributes = set()  # Track attributes removed from the original table\n",
    "\n",
    "        for table in all_tables:\n",
    "            is_original_table = (table['table_name'] == original_table_name)  # Check if this is the original table\n",
    "            bcnf_violation_found = False  # Reset for each table\n",
    "            relevant_fds = [\n",
    "                fd for fd in self.fds\n",
    "                if set(fd['lhs']).issubset(set(table['attributes'])) and set(fd['rhs']).issubset(set(table['attributes']))\n",
    "            ]\n",
    "\n",
    "            for fd in relevant_fds:\n",
    "                lhs, rhs = fd['lhs'], fd['rhs']\n",
    "                primary_key = table['primary_key']\n",
    "                \n",
    "                # Check for a BCNF violation (LHS is not a superkey)\n",
    "                if not set(lhs).issuperset(set(primary_key)):\n",
    "                    bcnf_violation_found = True\n",
    "                    print(f\"\\nTable {table['table_name']} violates BCNF (dependency: {fd}). Decomposing.\")\n",
    "\n",
    "                    new_relation = {\n",
    "                        'table_name': f\"{table['table_name']}_BCNF_{self.table_counter}\",\n",
    "                        'attributes': list(set(lhs + rhs)),\n",
    "                        'primary_key': lhs,\n",
    "                        'rows': self.Get_rows(table['rows'], lhs, rhs),\n",
    "                    }\n",
    "                    decomposed_tables.append(new_relation)\n",
    "                    self.normalized_relations.append(new_relation)\n",
    "                    self.table_counter += 1\n",
    "\n",
    "                    # Display the new decomposed table immediately after creation\n",
    "                    print(f\"Decomposed table {new_relation['table_name']}:\")\n",
    "                    print(\"Attributes:\", new_relation['attributes'])\n",
    "                    #print(\"Rows:\", new_relation['rows'])\n",
    "                    print(\"Primary Key:\", new_relation['primary_key'])\n",
    "\n",
    "                    nf_history['BCNF'].append(new_relation)\n",
    "\n",
    "                    # Remove RHS attributes of this dependency from the current table and track removed attributes\n",
    "                    for attr in rhs:\n",
    "                        if attr not in removed_attributes:\n",
    "                            removed_attributes.add(attr)\n",
    "                            table['attributes'].remove(attr)\n",
    "\n",
    "            # Update rows of the original table if a BCNF violation was found\n",
    "            if bcnf_violation_found and is_original_table:\n",
    "                print(f\"\\nUpdating original table {table['table_name']} after decomposition:\")\n",
    "                table['rows'] = [\n",
    "                    [value for i, value in enumerate(row) if i < len(table['attributes']) and table['attributes'][i] not in removed_attributes]\n",
    "                    for row in table['rows']\n",
    "                ]\n",
    "\n",
    "                # Output the updated original table after attribute removal\n",
    "                print(f\"Updated original table {table['table_name']}:\")\n",
    "                print(\"Attributes:\", table['attributes'])\n",
    "                print(\"Rows:\", table['rows'])\n",
    "                print(\"Primary Key:\", table['primary_key'])\n",
    "\n",
    "        # Return the decomposed tables or indicate all are in BCNF\n",
    "        if decomposed_tables:\n",
    "            print(\"\\nFinal decomposed tables in BCNF:\")\n",
    "            for decomposed_table in decomposed_tables:\n",
    "                print(f\"Table {decomposed_table['table_name']}:\")\n",
    "                print(\"Attributes:\", decomposed_table['attributes'])\n",
    "                #print(\"Rows:\", decomposed_table['rows'])\n",
    "                print(\"Primary Key:\", decomposed_table['primary_key'])\n",
    "        else:\n",
    "            print(\"All tables are in BCNF.\")\n",
    "\n",
    "        return decomposed_tables if decomposed_tables else False\n",
    "\n",
    "    def apply_4NF(self, relation, nf_history):\n",
    "        all_tables = [relation] + self.normalized_relations  # Include original and 1NF decompositions\n",
    "        decomposed_tables = []\n",
    "\n",
    "        for table in all_tables:\n",
    "            multi_value_dependency_found = False  # Reset for each table\n",
    "            candidate_keys = self.get_candidate_keys(table)  # Get candidate keys for the current table\n",
    "\n",
    "            for mvd in self.mvds:\n",
    "                lhs, rhs = mvd['lhs'], mvd['rhs']\n",
    "\n",
    "                # Check for a violation of 4NF\n",
    "                if not self.is_superkey(lhs, table['attributes'], candidate_keys):\n",
    "                    multi_value_dependency_found = True\n",
    "                    print(f\"\\nTable {table['table_name']} violates 4NF (MVD: {mvd}). Decomposing.\")\n",
    "\n",
    "                    # Create new relation based on MVD\n",
    "                    new_relation = {\n",
    "                        'table_name': f\"{table['table_name']}_4NF_{self.table_counter}\",\n",
    "                        'attributes': list(set(lhs + rhs)),\n",
    "                        'primary_key': lhs,\n",
    "                        'rows': self.Get_rows(table['rows'], lhs, rhs),\n",
    "                    }\n",
    "\n",
    "                    # Display the newly created relation's structure\n",
    "                    print(f\"Decomposed table {new_relation['table_name']}:\")\n",
    "                    print(\"Attributes:\", new_relation['attributes'])\n",
    "                    #print(\"Rows:\", new_relation['rows'])\n",
    "                    print(\"Primary Key:\", new_relation['primary_key'])\n",
    "\n",
    "                    decomposed_tables.append(new_relation)\n",
    "                    self.normalized_relations.append(new_relation)\n",
    "                    self.table_counter += 1\n",
    "\n",
    "                    nf_history['4NF'].append(new_relation)\n",
    "\n",
    "                    # Remove RHS attributes of this MVD from the current table\n",
    "                    table['attributes'] = [attr for attr in table['attributes'] if attr not in rhs]\n",
    "\n",
    "\n",
    "        return decomposed_tables if decomposed_tables else False\n",
    "\n",
    "    def get_candidate_keys(self, relation):\n",
    "      \n",
    "        candidate_keys = []\n",
    "        \n",
    "        # Start with primary key as a candidate key\n",
    "        primary_key = relation['primary_key']\n",
    "        candidate_keys.append(primary_key)\n",
    "\n",
    "        # Function to check if a set of attributes can determine other attributes\n",
    "        def can_determine(lhs, attributes):\n",
    "            closure = set(lhs)\n",
    "            while True:\n",
    "                new_attributes = closure.copy()\n",
    "                for fd in self.fds:\n",
    "                    if set(fd['lhs']).issubset(closure):\n",
    "                        new_attributes.update(fd['rhs'])\n",
    "                if new_attributes == closure:\n",
    "                    break\n",
    "                closure = new_attributes\n",
    "            return closure == set(attributes)\n",
    "\n",
    "        # Find all combinations of attributes that can serve as candidate keys\n",
    "        attributes = relation['attributes']\n",
    "        from itertools import combinations\n",
    "\n",
    "        # Iterate through all combinations of attributes\n",
    "        for r in range(1, len(attributes) + 1):\n",
    "            for combo in combinations(attributes, r):\n",
    "                # Check if the combo can determine all attributes\n",
    "                if can_determine(combo, attributes):\n",
    "                    # Check if the combo is minimal\n",
    "                    is_minimal = True\n",
    "                    for key in candidate_keys:\n",
    "                        if set(key).issubset(set(combo)):\n",
    "                            is_minimal = False\n",
    "                            break\n",
    "                    if is_minimal:\n",
    "                        candidate_keys.append(combo)\n",
    "\n",
    "        # Remove duplicates and non-minimal candidate keys\n",
    "        unique_candidate_keys = []\n",
    "        for key in candidate_keys:\n",
    "            if not any(set(key).issubset(set(other_key)) for other_key in unique_candidate_keys):\n",
    "                unique_candidate_keys.append(key)\n",
    "\n",
    "        return unique_candidate_keys\n",
    "\n",
    "\n",
    "    def is_superkey(self, lhs, attributes, candidate_keys):\n",
    "        # Calculate the closure of lhs\n",
    "        closure = set(lhs)\n",
    "        while True:\n",
    "            new_attributes = closure.copy()\n",
    "            for fd in self.fds:\n",
    "                if set(fd['lhs']).issubset(closure):\n",
    "                    new_attributes.update(fd['rhs'])\n",
    "            if new_attributes == closure:\n",
    "                break\n",
    "            closure = new_attributes\n",
    "\n",
    "        # If closure covers all attributes, lhs is a superkey\n",
    "        return closure == set(attributes)\n",
    "\n",
    "\n",
    "\n",
    "    def is_join_dependency_satisfied(self, lhs, rhs, table):\n",
    "        \n",
    "        # Filter out any attributes in rhs that are not present in the table\n",
    "        filtered_rhs = [attr for attr in rhs if attr in table['attributes']]\n",
    "        \n",
    "        if not filtered_rhs:  # If there are no valid RHS attributes, treat as satisfied\n",
    "            return True\n",
    "        \n",
    "        try:\n",
    "            # Check if the table satisfies the join dependency\n",
    "            lhs_indices = [table['attributes'].index(attr) for attr in lhs]\n",
    "            rhs_indices = [table['attributes'].index(attr) for attr in filtered_rhs]\n",
    "\n",
    "            # Logic to check if join dependency is satisfied (this may involve checking unique combinations)\n",
    "            lhs_values = {(tuple(row[i] for i in lhs_indices)) for row in table['rows']}\n",
    "            rhs_values = {(tuple(row[i] for i in rhs_indices)) for row in table['rows']}\n",
    "            \n",
    "            return len(lhs_values) == len(rhs_values)\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"Error checking join dependency {lhs} -> {filtered_rhs} for table {table['table_name']}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def apply_5NF(self, relation, nf_history):\n",
    "        \n",
    "        print(f\"\\nChecking table {relation['table_name']} for 5NF violations.\")\n",
    "\n",
    "        # Track whether any join dependencies have been violated\n",
    "        has_violations = False\n",
    "\n",
    "        # Loop through all previously normalized relations (decomposed tables)\n",
    "        for table in self.normalized_relations:\n",
    "            print(f\"Checking table {table['table_name']} for join dependencies.\")\n",
    "\n",
    "            for join_dep in self.join_dependencies:\n",
    "                lhs, rhs = join_dep['lhs'], join_dep['rhs']\n",
    "\n",
    "                # Check if any attributes in lhs or rhs are not in the current table's attributes\n",
    "                if not all(attr in table['attributes'] for attr in lhs) or not any(attr in table['attributes'] for attr in rhs):\n",
    "                    continue  # Skip this join dependency if it can't be checked\n",
    "\n",
    "                if not self.is_join_dependency_satisfied(lhs, rhs, table):\n",
    "                    print(f\"\\nTable {table['table_name']} violates 5NF (join dependency: {join_dep}). Decomposing.\")\n",
    "\n",
    "                    # Decompose the relation based on the join dependency\n",
    "                    new_tables = self.decompose_5NF(table, lhs, rhs)\n",
    "                    self.normalized_relations.extend(new_tables)  # Add new tables to normalized relations\n",
    "\n",
    "                    has_violations = True\n",
    "                    break  # Break after the first violation and recheck the same table\n",
    "\n",
    "            if has_violations:\n",
    "                break  # Exit the loop if we found any violations\n",
    "\n",
    "        if not has_violations:\n",
    "            print(f\"All join dependencies satisfied for table {relation['table_name']}.\")\n",
    "\n",
    "\n",
    "    def decompose_5NF(self, table, lhs, rhs):\n",
    "       \n",
    "        decomposed_tables = []\n",
    "        \n",
    "        # Gather unique combinations of lhs values\n",
    "        lhs_values = set(tuple(row[table['attributes'].index(attr)] for attr in lhs) for row in table['rows'])\n",
    "        \n",
    "        for value in lhs_values:\n",
    "            # Create a new table for each unique value of lhs\n",
    "            decomposed_table = {\n",
    "                'table_name': f\"{table['table_name']}_5NF_{value}\",\n",
    "                'attributes': list(rhs) + list(lhs),\n",
    "                'rows': [],\n",
    "                'primary_key': list(lhs)  # Assuming lhs is the primary key in the new table\n",
    "            }\n",
    "            \n",
    "            # Populate the new table's rows\n",
    "            for row in table['rows']:\n",
    "                if tuple(row[table['attributes'].index(attr)] for attr in lhs) == value:\n",
    "                    new_row = [row[table['attributes'].index(attr)] for attr in rhs] + list(value)\n",
    "                    decomposed_table['rows'].append(new_row)\n",
    "            \n",
    "            decomposed_tables.append(decomposed_table)\n",
    "            nf_history['5NF'].append(new_relation)\n",
    "\n",
    "        return decomposed_tables if decomposed_tables else False\n",
    "\n",
    "    def create_join_table_rows(self, lhs, rhs, table):\n",
    "       \n",
    "        lhs_indices = [table['attributes'].index(attr) for attr in lhs]\n",
    "        rhs_indices = [table['attributes'].index(attr) for attr in rhs]\n",
    "        \n",
    "        new_rows = []\n",
    "        for row in table['rows']:\n",
    "            new_row = [row[idx] for idx in lhs_indices] + [row[idx] for idx in rhs_indices]\n",
    "            new_rows.append(new_row)\n",
    "        \n",
    "        return new_rows\n",
    "\n",
    "    def validate_mvd(self, mvd, relation):\n",
    "       \n",
    "        lhs, rhs = mvd[\"lhs\"], mvd[\"rhs\"]\n",
    "        return True  # Return True if MVD is valid, otherwise False\n",
    "\n",
    "    def print_table(self, relation, step_description):\n",
    "        \n",
    "        print(f\"\\n{step_description} for {relation['table_name']}:\")\n",
    "        df = pd.DataFrame(relation['rows'], columns=relation['attributes'])\n",
    "        print(df.to_string(index=False))  # Print without row index\n",
    "\n",
    "def parse_input(file_path: str):\n",
    "   \n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    relations = data[\"relations\"]\n",
    "    fds = data[\"functional_dependencies\"]\n",
    "    mvds = data[\"mvds\"]\n",
    "    target_nf = data[\"target_normal_form\"]\n",
    "    return relations, fds, mvds, target_nf\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ask user for the input file path\n",
    "    file_path = input(\"Enter the JSON file name (in the current directory): \")\n",
    "    \n",
    "    try:\n",
    "        relations, fds, mvds, target_nf = parse_input(file_path)\n",
    "        normalizer = Normalizer(relations, fds, mvds, target_nf)\n",
    "        normalized_relations = normalizer.normalize()\n",
    "\n",
    "      \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baf0fa1-2968-4262-bc01-018f051dc149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
