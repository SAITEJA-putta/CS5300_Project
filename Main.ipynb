{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a440d4e2-3c50-49ca-a8f2-9125e13b2f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from TestingData (1NF-5NF).xlsx...\n",
      "Data loaded successfully. Shape: (5, 20)\n",
      "Initial data: \n",
      "  Unnamed: 0           Unnamed: 1            Unnamed: 2 Unnamed: 3  \\\n",
      "0    OrderID                 Date         PromocodeUsed  TotalCost   \n",
      "1       1001  2024-06-30 00:00:00                  NONE       7.25   \n",
      "2       1002  2026-06-30 00:00:00             SUMMERFUN       9.98   \n",
      "3       1002  2026-06-30 00:00:00             SUMMERFUN       9.98   \n",
      "4       1003  2024-06-29 00:00:00  {SUMMERFUN, JUNEVIP}        115   \n",
      "\n",
      "       Unnamed: 4     Unnamed: 5  Unnamed: 6    Unnamed: 7 Unnamed: 8  \\\n",
      "0  TotalDrinkCost  TotalFoodCost  CustomerID  CustomerName    DrinkID   \n",
      "1            7.25              0           1   Alice Brown          1   \n",
      "2            5.99           3.99           2  David Miller          2   \n",
      "3            5.99           3.99           2  David Miller          3   \n",
      "4             115              0           3  Emily Garcia          4   \n",
      "\n",
      "                 Unnamed: 9 Unnamed: 10    Unnamed: 11 Unnamed: 12  \\\n",
      "0                 DrinkName   DrinkSize  DrinkQuantity        Milk   \n",
      "1               Caffe Latte      Grande              1          ND   \n",
      "2    Iced Caramel Macchiato        Tall              2           D   \n",
      "3         Iced Matcha Latte      Grande              1          ND   \n",
      "4  Vanilla Bean Frappuccino       Venti              8          ND   \n",
      "\n",
      "                              Unnamed: 13    Unnamed: 14 Unnamed: 15  \\\n",
      "0                         DrinkIngredient  DrinkAllergen      FoodID   \n",
      "1                    {Espresso, Oat Milk}         {Oat}            0   \n",
      "2    {Expresso, Vanilla Syrup, Milk, Ice}  {Dairy, Nuts}           3   \n",
      "3            {Matcha, Coconut Milk, Ice}          {Nuts}           3   \n",
      "4  {Coffee, Ice, Vanilla Syrup, Soy Milk}    {Nuts, Soy}           0   \n",
      "\n",
      "         Unnamed: 16   Unnamed: 17                        Unnamed: 18  \\\n",
      "0           FoodName  FoodQuantity                     FoodIngredient   \n",
      "1                NaN             0                               NONE   \n",
      "2  Blueberry Muffin              1  {Flour, Sugar, Blueberries, Eggs}   \n",
      "3  Blueberry Muffin              1  {Flour, Sugar, Blueberries, Eggs}   \n",
      "4                NaN             0                               NONE   \n",
      "\n",
      "    Unnamed: 19  \n",
      "0  FoodAllergen  \n",
      "1          NONE  \n",
      "2  {Wheat, Egg}  \n",
      "3  {Wheat, Egg}  \n",
      "4          NONE  \n",
      "\n",
      "Functional Dependencies from file: [(['OrderID'], ['Date', 'TotalCost', 'TotalDrinkCost', 'TotalFoodCost', 'CustomerID', 'CustomerName']), (['OrderID', 'DrinkID'], ['DrinkSize', 'DrinkQuantity', 'Milk']), (['OrderID', 'FoodID'], ['FoodQuantity']), (['CustomerID'], ['CustomerName']), (['DrinkID'], ['DrinkName']), (['FoodID'], ['FoodName'])]\n",
      "Multi-valued Dependencies from file: [(['OrderID'], ['DrinkID']), (['OrderID'], ['FoodID'])]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the primary keys (can be composite, separated by commas, no spaces between; multiple keys separated by semicolons):  OrderID,DrinkID,FoodID\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided Primary Keys: [['OrderID', 'DrinkID', 'FoodID']]\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the highest normalization form (1NF, 2NF, 3NF, BCNF, 4NF, 5NF):  BCNF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1NF Violations identified:\n",
      "Column 'PromocodeUsed' contains multiple values per field:\n",
      "  Row 3: {SUMMERFUN, JUNEVIP}\n",
      "Column 'DrinkIngredient' contains multiple values per field:\n",
      "  Row 0: {Espresso, Oat Milk}\n",
      "  Row 1: {Expresso, Vanilla Syrup, Milk, Ice}\n",
      "  Row 2: {Matcha, Coconut Milk, Ice} \n",
      "  Row 3: {Coffee, Ice, Vanilla Syrup, Soy Milk}\n",
      "Column 'DrinkAllergen' contains multiple values per field:\n",
      "  Row 1: {Dairy, Nuts}\n",
      "  Row 3: {Nuts, Soy}\n",
      "Column 'FoodIngredient' contains multiple values per field:\n",
      "  Row 1: {Flour, Sugar, Blueberries, Eggs}\n",
      "  Row 2: {Flour, Sugar, Blueberries, Eggs}\n",
      "Column 'FoodAllergen' contains multiple values per field:\n",
      "  Row 1: {Wheat, Egg}\n",
      "  Row 2: {Wheat, Egg}\n",
      "\n",
      "Converting to 2NF...\n",
      "Converting to 3NF...\n",
      "Converting to BCNF...\n",
      "Normalized Database Schema:\n",
      "Relation_1 (CustomerID, CustomerName)\n",
      "PK: CustomerID\n",
      "\n",
      "Relation_2 (OrderID, Date, TotalCost, TotalDrinkCost, TotalFoodCost, CustomerID)\n",
      "PK: OrderID\n",
      "\n",
      "Relation_3 (OrderID, DrinkID, DrinkSize, DrinkQuantity, Milk)\n",
      "PK: OrderID, DrinkID\n",
      "\n",
      "Relation_4 (OrderID, FoodID, FoodQuantity)\n",
      "PK: OrderID, FoodID\n",
      "\n",
      "Relation_5 (DrinkID, DrinkName)\n",
      "PK: DrinkID\n",
      "\n",
      "Relation_6 (FoodID, FoodName)\n",
      "PK: FoodID\n",
      "\n",
      "Relation_7 (OrderID, DrinkID, FoodID)\n",
      "PK: OrderID, DrinkID, FoodID\n",
      "\n",
      "Relation_8 (OrderID, DrinkID, FoodID, PromocodeUsed)\n",
      "PK: OrderID, DrinkID, FoodID, PromocodeUsed\n",
      "\n",
      "Relation_9 (OrderID, DrinkID, FoodID, DrinkIngredient)\n",
      "PK: OrderID, DrinkID, FoodID, DrinkIngredient\n",
      "\n",
      "Relation_10 (OrderID, DrinkID, FoodID, DrinkAllergen)\n",
      "PK: OrderID, DrinkID, FoodID, DrinkAllergen\n",
      "\n",
      "Relation_11 (OrderID, DrinkID, FoodID, FoodIngredient)\n",
      "PK: OrderID, DrinkID, FoodID, FoodIngredient\n",
      "\n",
      "Relation_12 (OrderID, DrinkID, FoodID, FoodAllergen)\n",
      "PK: OrderID, DrinkID, FoodID, FoodAllergen\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict\n",
    "from itertools import combinations\n",
    "\n",
    "def load_data(file_path: str) -> pd.DataFrame:\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    data_frame = pd.read_excel(file_path)\n",
    "    print(f\"Data loaded successfully. Shape: {data_frame.shape}\")\n",
    "    return data_frame\n",
    "\n",
    "def load_fds_from_file(file_path: str) -> List[Tuple[List[str], List[str]]]:\n",
    "    functional_dependencies = []\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                if '-->' in line:\n",
    "                    left_attrs, right_attrs = line.strip().split(\"-->\")\n",
    "                    lhs = [attr.strip() for attr in left_attrs.split(\",\")]\n",
    "                    rhs = [attr.strip() for attr in right_attrs.split(\",\")]\n",
    "                    functional_dependencies.append((lhs, rhs))\n",
    "    except Exception as error:\n",
    "        print(f\"Error loading FDs from file {file_path}: {error}\")\n",
    "    return functional_dependencies\n",
    "\n",
    "def load_mvds_from_file(file_path: str) -> List[Tuple[List[str], List[str]]]:\n",
    "    multivalued_dependencies = []\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                if '->>' in line:\n",
    "                    left_attrs, right_attrs = line.strip().split(\"->>\")\n",
    "                    lhs = [attr.strip() for attr in left_attrs.split(\",\")]\n",
    "                    rhs = [attr.strip() for attr in right_attrs.split(\",\")]\n",
    "                    multivalued_dependencies.append((lhs, rhs))\n",
    "    except Exception as error:\n",
    "        print(f\"Error loading MVDs from file {file_path}: {error}\")\n",
    "    return multivalued_dependencies\n",
    "\n",
    "def detect_multivalued_columns(data_frame: pd.DataFrame) -> Tuple[List[str], Dict[str, List[Tuple[int, str]]]]:\n",
    "    data_frame.columns = data_frame.iloc[0]\n",
    "    data_frame = data_frame.drop(data_frame.index[0]).reset_index(drop=True)\n",
    "    \n",
    "    multivalued_columns = []\n",
    "    validation_issues = {}\n",
    "\n",
    "    for column in data_frame.columns:\n",
    "        column_issues = []\n",
    "        for idx, value in data_frame[column].items():\n",
    "            if isinstance(value, str) and ',' in value:\n",
    "                column_issues.append((idx, value))\n",
    "        if column_issues:\n",
    "            multivalued_columns.append(column)\n",
    "            validation_issues[column] = column_issues\n",
    "\n",
    "    return multivalued_columns, validation_issues\n",
    "\n",
    "def to_1nf(data_frame: pd.DataFrame, primary_keys: List[List[str]]) -> List[Tuple[pd.DataFrame, List[str]]]:\n",
    "    multivalued_columns, validation_issues = detect_multivalued_columns(data_frame)\n",
    "\n",
    "    if validation_issues:\n",
    "        print(\"1NF Violations identified:\")\n",
    "        for column, issues in validation_issues.items():\n",
    "            print(f\"Column '{column}' contains multiple values per field:\")\n",
    "            for idx, value in issues:\n",
    "                print(f\"  Row {idx}: {value}\")\n",
    "        print()\n",
    "\n",
    "    base_table = data_frame.drop(columns=multivalued_columns, errors='ignore')\n",
    "    normalized_tables = [(base_table, primary_keys[0])]\n",
    "\n",
    "    for multivalued_column in multivalued_columns:\n",
    "        for primary_key in primary_keys:\n",
    "            normalized_table = data_frame[list(primary_key) + [multivalued_column]].copy()\n",
    "            normalized_table[multivalued_column] = normalized_table[multivalued_column].apply(lambda x: x.split(',') if isinstance(x, str) else x)\n",
    "            normalized_table = normalized_table.explode(multivalued_column).dropna().drop_duplicates()\n",
    "            normalized_tables.append((normalized_table, primary_key + [multivalued_column]))\n",
    "\n",
    "    return normalized_tables\n",
    "\n",
    "def to_2nf(tables: List[Tuple[pd.DataFrame, List[str]]], fds: List[Tuple[List[str], List[str]]]) -> Tuple[List[Tuple[pd.DataFrame, List[str]]], List[Tuple[List[str], List[str]]]]:\n",
    "    print(\"Converting to 2NF...\")\n",
    "    if not fds:\n",
    "        print(\"No functional dependencies available.\")\n",
    "        return tables, fds\n",
    "\n",
    "    updated_tables = []\n",
    "    remaining_fds = fds.copy()\n",
    "\n",
    "    for table, primary_key in tables:\n",
    "        partial_dependencies = []\n",
    "        for lhs, rhs in remaining_fds:\n",
    "            if set(lhs).issubset(primary_key) and not set(lhs) == set(primary_key):\n",
    "                if len(lhs) == 1 and len(rhs) == 1 and rhs[0] not in primary_key:\n",
    "                    continue\n",
    "                partial_dependencies.append((lhs, rhs))\n",
    "\n",
    "        for lhs, rhs in partial_dependencies:\n",
    "            columns_to_keep = [col for col in (lhs + rhs) if col in table.columns]\n",
    "            normalized_table = table[columns_to_keep].drop_duplicates()\n",
    "            updated_tables.append((normalized_table, lhs))\n",
    "            table = table.drop(columns=[col for col in rhs if col in table.columns]).drop_duplicates()\n",
    "            remaining_fds.remove((lhs, rhs))\n",
    "\n",
    "        remaining_columns = list(set(table.columns) - set(primary_key))\n",
    "        remaining_columns = [col for col in remaining_columns if col in table.columns]\n",
    "        if remaining_columns:\n",
    "            remaining_table = table[primary_key + remaining_columns].drop_duplicates()\n",
    "            updated_tables.append((remaining_table, primary_key))\n",
    "        else:\n",
    "            updated_tables.append((table, primary_key))\n",
    "\n",
    "    return updated_tables, remaining_fds\n",
    "\n",
    "def to_3nf(tables: List[Tuple[pd.DataFrame, List[str]]], fds: List[Tuple[List[str], List[str]]]) -> Tuple[List[Tuple[pd.DataFrame, List[str]]], List[Tuple[List[str], List[str]]]]:\n",
    "    print(\"Converting to 3NF...\")\n",
    "    if not fds:\n",
    "        print(\"No functional dependencies available.\")\n",
    "        return tables, fds\n",
    "\n",
    "    transitive_dependencies = []\n",
    "    for lhs, rhs in fds:\n",
    "        for table, primary_key in tables:\n",
    "            if set(lhs).issubset(table.columns) and not set(lhs).issubset(primary_key):\n",
    "                if not any(attr in primary_key for attr in lhs) and not any(attr in primary_key for attr in rhs):\n",
    "                    transitive_dependencies.append((lhs, rhs))\n",
    "\n",
    "    updated_tables = []\n",
    "    remaining_fds = fds.copy()\n",
    "    for table, primary_key in tables:\n",
    "        columns = set(table.columns)\n",
    "        table_fds = [fd for fd in fds if set(fd[0]).issubset(columns) and set(fd[1]).issubset(columns)]\n",
    "\n",
    "        for lhs, rhs in table_fds:\n",
    "            if (lhs, rhs) in transitive_dependencies:\n",
    "                new_table = table[lhs + rhs].drop_duplicates()\n",
    "                updated_tables.append((new_table, lhs))\n",
    "                table = table.drop(columns=[col for col in rhs if col in table.columns])\n",
    "                remaining_fds.remove((lhs, rhs))\n",
    "\n",
    "        updated_tables.append((table, primary_key))\n",
    "\n",
    "    return updated_tables, remaining_fds\n",
    "\n",
    "def to_bcnf(tables: List[Tuple[pd.DataFrame, List[str]]], fds: List[Tuple[List[str], List[str]]], primary_keys: List[List[str]]) -> Tuple[List[Tuple[pd.DataFrame, List[str]]], List[Tuple[List[str], List[str]]]]:\n",
    "    print(\"Converting to BCNF...\")\n",
    "    if not fds:\n",
    "        print(\"No functional dependencies available.\")\n",
    "        return tables, fds\n",
    "\n",
    "    def is_superkey(attrs: List[str], primary_keys: List[List[str]]) -> bool:\n",
    "        for key in primary_keys:\n",
    "            if set(key).issubset(set(attrs)):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    updated_tables = []\n",
    "    remaining_fds = fds.copy()\n",
    "    bcnf_violations = []\n",
    "\n",
    "    for table, primary_key in tables:\n",
    "        columns = set(table.columns)\n",
    "        table_fds = [fd for fd in fds if set(fd[0]).issubset(columns) and set(fd[1]).issubset(columns)]\n",
    "\n",
    "        for lhs, rhs in table_fds:\n",
    "            if not is_superkey(lhs, primary_keys):\n",
    "                bcnf_violations.append((lhs, rhs))\n",
    "\n",
    "                new_primary_key = lhs\n",
    "                if not any(set(new_primary_key).issubset(set(key)) for key in primary_keys):\n",
    "                    primary_keys.append(new_primary_key)\n",
    "\n",
    "                new_table = table[lhs + rhs].drop_duplicates()\n",
    "                updated_tables.append((new_table, lhs))\n",
    "                table = table.drop(columns=[col for col in rhs if col in table.columns])\n",
    "                remaining_fds.remove((lhs, rhs))\n",
    "\n",
    "        updated_tables.append((table, primary_key))\n",
    "\n",
    "    return updated_tables, remaining_fds\n",
    "\n",
    "def to_4nf(tables: List[Tuple[pd.DataFrame, List[str]]], mvds: List[Tuple[List[str], List[str]]], primary_keys: List[List[str]]) -> List[Tuple[pd.DataFrame, List[str]]]:\n",
    "    print(\"Converting to 4NF...\")\n",
    "    if not mvds:\n",
    "        print(\"No multivalued dependencies available.\")\n",
    "        return tables\n",
    "\n",
    "    updated_tables = []\n",
    "    for table, primary_key in tables:\n",
    "        columns = set(table.columns)\n",
    "        for lhs, rhs in mvds:\n",
    "            if set(rhs).issubset(columns):\n",
    "                if not any(set(lhs).issubset(set(key)) for key in primary_keys):\n",
    "                    new_table = table[lhs + rhs].drop_duplicates()\n",
    "                    updated_tables.append((new_table, lhs))\n",
    "                    table = table.drop(columns=[col for col in rhs if col in table.columns])\n",
    "        updated_tables.append((table, primary_key))\n",
    "    return updated_tables\n",
    "\n",
    "def to_5nf(tables: List[Tuple[pd.DataFrame, List[str]]], primary_keys: List[List[str]]) -> List[Tuple[pd.DataFrame, List[str]]]:\n",
    "    print(\"Converting to 5NF...\")\n",
    "\n",
    "    def check_join_dependency(df: pd.DataFrame, primary_keys: List[List[str]]) -> bool:\n",
    "        for primary_key in primary_keys:\n",
    "            non_key_attrs = list(set(df.columns) - set(primary_key))\n",
    "            for i in range(1, len(non_key_attrs) + 1):\n",
    "                for subset in combinations(non_key_attrs, i):\n",
    "                    lhs = list(primary_key) + list(subset)\n",
    "                    rhs = list(set(df.columns) - set(lhs))\n",
    "                    if set(lhs).issubset(df.columns) and set(rhs).issubset(df.columns):\n",
    "                        if df[lhs + rhs].drop_duplicates().shape[0] != df.drop_duplicates().shape[0]:\n",
    "                            return True\n",
    "        return False\n",
    "\n",
    "    final_tables = []\n",
    "    for table, primary_key in tables:\n",
    "        if check_join_dependency(table, primary_keys):\n",
    "            for primary_key in primary_keys:\n",
    "                non_key_attrs = list(set(table.columns) - set(primary_key))\n",
    "                for i in range(1, len(non_key_attrs) + 1):\n",
    "                    for subset in combinations(non_key_attrs, i):\n",
    "                        lhs = list(primary_key) + list(subset)\n",
    "                        rhs = list(set(table.columns) - set(lhs))\n",
    "                        if set(lhs).issubset(table.columns) and set(rhs).issubset(table.columns):\n",
    "                            if table[lhs + rhs].drop_duplicates().shape[0] != table.drop_duplicates().shape[0]:\n",
    "                                new_table_lhs = table[lhs].drop_duplicates()\n",
    "                                new_table_rhs = table[rhs].drop_duplicates()\n",
    "                                final_tables.append((new_table_lhs, primary_key))\n",
    "                                final_tables.append((new_table_rhs, rhs))\n",
    "                                break\n",
    "                    else:\n",
    "                        continue\n",
    "                    break\n",
    "            else:\n",
    "                final_tables.append((table, primary_key))\n",
    "        else:\n",
    "            final_tables.append((table, primary_key))\n",
    "\n",
    "    return final_tables\n",
    "\n",
    "def compile_schema(data_tables: List[Tuple[pd.DataFrame, List[str]]], primary_keys: List[List[str]]) -> str:\n",
    "    schema_text = \"\"\n",
    "    for i, (table_df, primary_key) in enumerate(data_tables):\n",
    "        if isinstance(table_df, pd.DataFrame):\n",
    "            table_name = f\"Relation_{i+1}\"\n",
    "            table_df.columns = table_df.iloc[0]\n",
    "            columns_list = \", \".join(map(str, table_df.columns))\n",
    "            primary_key_str = \", \".join(primary_key)\n",
    "\n",
    "            reordered_columns = [col for col in table_df.columns if col in primary_key] + \\\n",
    "                                [col for col in table_df.columns if col not in primary_key]\n",
    "            columns_list = \", \".join(reordered_columns)\n",
    "\n",
    "            schema_text += f\"{table_name} ({columns_list})\\n\"\n",
    "            schema_text += f\"PK: {primary_key_str}\\n\\n\"\n",
    "        else:\n",
    "            print(f\"Error: Expected DataFrame but got {type(table_df)}\")\n",
    "    return schema_text\n",
    "\n",
    "def main():\n",
    "    data_file = 'TestingData (1NF-5NF).xlsx'\n",
    "    fd_file = 'fds.txt'\n",
    "    mvd_file = 'mvd.txt'\n",
    "\n",
    "    data_frame = load_data(data_file)\n",
    "    print(f\"Initial data: \\n{data_frame.head()}\\n\")\n",
    "\n",
    "    fds = load_fds_from_file(fd_file)\n",
    "    print(f\"Functional Dependencies from file: {fds}\")\n",
    "\n",
    "    mvds = load_mvds_from_file(mvd_file)\n",
    "    print(f\"Multi-valued Dependencies from file: {mvds}\")\n",
    "\n",
    "    primary_keys_input = input(\"Enter the primary keys (can be composite, separated by commas, no spaces between; multiple keys separated by semicolons): \")\n",
    "    primary_keys = [key.strip().split(',') for key in primary_keys_input.split(';')]\n",
    "    print(f\"Provided Primary Keys: {primary_keys}\\n\")\n",
    "\n",
    "    highest_normal_form = input(\"Enter the highest normalization form (1NF, 2NF, 3NF, BCNF, 4NF, 5NF): \").upper()\n",
    "\n",
    "    tables = to_1nf(data_frame, primary_keys)\n",
    "\n",
    "    if highest_normal_form == \"1NF\":\n",
    "        schema = compile_schema(tables, primary_keys)\n",
    "        print(\"Normalized Database Schema:\")\n",
    "        print(schema)\n",
    "        return\n",
    "\n",
    "    if highest_normal_form in [\"2NF\", \"3NF\", \"BCNF\", \"4NF\", \"5NF\"]:\n",
    "        tables, fds = to_2nf(tables, fds)\n",
    "\n",
    "    if highest_normal_form in [\"3NF\", \"BCNF\", \"4NF\", \"5NF\"]:\n",
    "        tables, fds = to_3nf(tables, fds)\n",
    "\n",
    "    if highest_normal_form in [\"BCNF\", \"4NF\", \"5NF\"]:\n",
    "        tables, fds = to_bcnf(tables, fds, primary_keys)\n",
    "\n",
    "    if highest_normal_form in [\"4NF\", \"5NF\"]:\n",
    "        tables = to_4nf(tables, mvds, primary_keys)\n",
    "\n",
    "    if highest_normal_form == \"5NF\":\n",
    "        tables = to_5nf(tables, primary_keys)\n",
    "\n",
    "    schema = compile_schema(tables, primary_keys)\n",
    "    print(\"Normalized Database Schema:\")\n",
    "    print(schema)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39188fd9-44dd-4452-a1f5-631329850c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
